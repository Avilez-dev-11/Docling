{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccb6e4d",
   "metadata": {},
   "source": [
    "# Docling as MCP tool with Llama Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c4bb8",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd21e20",
   "metadata": {},
   "source": [
    "### Technologies We'll Use\n",
    "\n",
    "Building on our previous labs, we'll add:\n",
    "\n",
    "1. **[Docling](https://docling-project.github.io/docling/):** An open-source toolkit used to parse and convert documents.\n",
    "2. **[MCP](https://modelcontextprotocol.io)**: The model context protocol for creating a tool.\n",
    "3. **[Llama Stack](https://llama-stack.readthedocs.io/)**: Framework for building generative AI applications.\n",
    "4. Agentic RAG: Use reasing and tools for an enhanced RAG flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12622cca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ade1e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we begin, ensure you have:\n",
    "- Completed Labs 1 (or equivalent Docling knowledge)\n",
    "- Python >=3.10 installed\n",
    "- Ollama installed\n",
    "- Podman installed (or Docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcf6a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12afefd",
   "metadata": {},
   "source": [
    "## Installation and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \\\n",
    "    llama-stack-client \\\n",
    "    pydantic \\\n",
    "    pydantic_settings \\\n",
    "    rich\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0447576",
   "metadata": {},
   "source": [
    "Now let's import the essential modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from pydantic import NonNegativeFloat\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "from rich.console import Console\n",
    "from rich.pretty import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb231c7",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf694d4",
   "metadata": {},
   "source": [
    "To see detailed information about the document processing and chunking operations, we'll configure INFO log level.\n",
    "\n",
    "NOTE: It is okay to skip running this cell if you prefer less verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb01a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.hasHandlers():  \n",
    "    logger.setLevel(logging.INFO)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d862854",
   "metadata": {},
   "source": [
    "### Setting up\n",
    "\n",
    "In the following blocks we setup the environment needed for connecting to llama stack and use it as agentic framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b141a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings(BaseSettings):\n",
    "    base_url: str\n",
    "\n",
    "    vdb_provider: str\n",
    "    vdb_embedding: str\n",
    "    vdb_embedding_dimension: int\n",
    "    vdb_embedding_window: int\n",
    "\n",
    "    inference_model_id: str\n",
    "    max_tokens: int\n",
    "    temperature: NonNegativeFloat\n",
    "    top_p: float\n",
    "    stream: bool\n",
    "\n",
    "    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_url='http://localhost:8321' vdb_provider='milvus' vdb_embedding='all-MiniLM-L6-v2' vdb_embedding_dimension=384 vdb_embedding_window=256 inference_model_id='meta-llama/Llama-3.2-3B-Instruct' max_tokens=4096 temperature=0.0 top_p=0.95 stream=True\n"
     ]
    }
   ],
   "source": [
    "settings = Settings(\n",
    "    base_url=\"http://localhost:8321\",\n",
    "    inference_model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    "    top_p=0.95,\n",
    "    stream=True,\n",
    "    vdb_provider=\"faiss\",\n",
    "    vdb_embedding=\"all-MiniLM-L6-v2\",\n",
    "    vdb_embedding_dimension=384,\n",
    "    vdb_embedding_window=256,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9171a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n"
     ]
    }
   ],
   "source": [
    "if settings.temperature > 0.0:\n",
    "    strategy = {\n",
    "        \"type\": \"top_p\",\n",
    "        \"temperature\": settings.temperature,\n",
    "        \"top_p\": settings.top_p,\n",
    "    }\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": settings.max_tokens,\n",
    "}\n",
    "\n",
    "print(sampling_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c87c25",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f1548",
   "metadata": {},
   "source": [
    "## Launch Llama Stack\n",
    "\n",
    "Within this lab we will interact with a Llama Stack backend. We have chosen the Ollama distribution which allows to easily get started on a local environment.\n",
    "\n",
    "### Fetch the models\n",
    "\n",
    "In a terminal window use the following command for fetching the models required for running.\n",
    "\n",
    "```bash\n",
    "export INFERENCE_MODEL=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# ollama names this model differently, and we must use the ollama name when loading the model\n",
    "export OLLAMA_INFERENCE_MODEL=\"llama3.2:3b-instruct-fp16\"\n",
    "ollama run $OLLAMA_INFERENCE_MODEL --keepalive 60m\n",
    "  ```\n",
    "\n",
    "\n",
    "### Start the Llama Stack container\n",
    "\n",
    "In a new terminal window use the following command to run the Llama Stack server.\n",
    "\n",
    "```bash\n",
    "# make a working directory which will be used by the container\n",
    "mkdir -p ~/.llama\n",
    "\n",
    "# launch llama stack\n",
    "export LLAMA_STACK_PORT=8321\n",
    "podman run \\\n",
    "  -it \\\n",
    "  --pull always \\\n",
    "  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n",
    "  -v ~/.llama:/root/.llama \\\n",
    "  llamastack/distribution-ollama \\\n",
    "  --port $LLAMA_STACK_PORT \\\n",
    "  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n",
    "  --env OLLAMA_URL=http://host.containers.internal:11434\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03360750",
   "metadata": {},
   "source": [
    "Next we can use the `LlamaStackClient` within this notebook validate the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99d42db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server @ http://localhost:8321\n"
     ]
    }
   ],
   "source": [
    "client = LlamaStackClient(base_url=settings.base_url)\n",
    "print(f\"Connected to Llama Stack server @ {client.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dcba84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d54948",
   "metadata": {},
   "source": [
    "## Launch the MCP tool\n",
    "\n",
    "MCP allows to connect custom tools (like Docling) within an agentic framework. In this lab we will use an MCP tool which allows to\n",
    "1. Convert documents using Docling\n",
    "2. Ingest them into a Llama Stack vector DB instance.\n",
    "\n",
    "_You can inspect how a tool is created by looking at the file [Docling_Lab4_tool.py](./Docling_Lab4_tool.py)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14983d2a",
   "metadata": {},
   "source": [
    "We already packaged the tool into a working container image which is ready for you to try out.\n",
    "\n",
    "**Launch the Docling Llama Stack MCP tool** by running the following command in a new terminal window.\n",
    "\n",
    "```bash\n",
    "podman run \\\n",
    "  -it \\\n",
    "  --pull always \\\n",
    "  -p 8000:8000 \\\n",
    "  ghrc.io/docling-project/lab-demo-docling-llamstack-mcp \\\n",
    "  --env DOCLING_MCP_LLAMA_STACK_URL=http://host.containers.internal:8321\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c3b7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26d641",
   "metadata": {},
   "source": [
    "### Validate tools available in our llama-stack instance\n",
    "\n",
    "When an instance of llama-stack is redeployed your tools need to re-registered. Also if a tool is already registered with a llama-stack instance, if you try to register one with the same `toolgroup_id`, llama-stack will throw you an error.\n",
    "\n",
    "For this reason it is recommended to include some code to validate your tools and toolgroups. This is where the `mcp_url` comes into play. The following code will check that the `mcp::docling-llamastack` tool is registered, or it will be registered directly from the mcp url.\n",
    "\n",
    "If you are running the MCP server from source, the default value for this is: `http://localhost:8000/sse`.\n",
    "\n",
    "If you are running the MCP server from a container, the default value for this is: `http://host.containers.internal:8000/sse`.\n",
    "\n",
    "Make sure to pass the corresponding MCP URL for the server you are trying to register/validate tools for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86dd3787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your Llama Stack server is already registered with the following tool groups @ {'mcp::docling-llamastack', 'builtin::rag', 'builtin::websearch'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docling_mcp_url = \"http://localhost:8000/sse\"\n",
    "\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "\n",
    "if \"mcp::docling-llamastack\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::docling-llamastack\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":docling_mcp_url},\n",
    "    )\n",
    "\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "logger.info(f\"Your Llama Stack server is already registered with the following tool groups @ {set(registered_toolgroups)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f79fb",
   "metadata": {},
   "source": [
    "## Ingest + RAG-aware agent\n",
    "\n",
    "- Initialize the collection in the vectordb\n",
    "- Initialize the agent the required tools:\n",
    "    - Docling Ingest will be responsible to take care of instructions like \"Ingest the document https://arxiv.org/pdf/2503.11576\".\n",
    "    - RAG/Knowledge search will respond to user queries by running RAG on the documents ingested in the vectordb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "126fe883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb2fcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the vectordb collection to use\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=settings.vdb_embedding,\n",
    "    embedding_dimension=settings.vdb_embedding_dimension,\n",
    "    provider_id=settings.vdb_provider,\n",
    ")\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=settings.inference_model_id,\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    sampling_params=sampling_params,\n",
    "    tools=[\n",
    "        dict(\n",
    "            name=\"mcp::docling-llamastack\",\n",
    "            args={\n",
    "                \"vector_db_id\": vector_db_id,\n",
    "            },\n",
    "        ),\n",
    "        dict(\n",
    "            name=\"builtin::rag/knowledge_search\",\n",
    "            args={\n",
    "                \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c653cc3",
   "metadata": {},
   "source": [
    "## Executing ingest and RAG queries\n",
    "\n",
    "- For each prompt, initialize a new agent session, execute a turn during which a retrieval call may be requested, and output the reply received from the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "374d7929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">User&gt; Ingest the document </span><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://arxiv.org/pdf/2503.11576</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mUser> Ingest the document \u001b[0m\u001b[4;36mhttps://arxiv.org/pdf/2503.11576\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[\u001b[0m\u001b[33ming\u001b[0m\u001b[33mest\u001b[0m\u001b[33m_document\u001b[0m\u001b[33m_to\u001b[0m\u001b[33m_vect\u001b[0m\u001b[33mord\u001b[0m\u001b[33mb\u001b[0m\u001b[33m(source\u001b[0m\u001b[33m='\u001b[0m\u001b[33mhttps\u001b[0m\u001b[33m://\u001b[0m\u001b[33mar\u001b[0m\u001b[33mxiv\u001b[0m\u001b[33m.org\u001b[0m\u001b[33m/pdf\u001b[0m\u001b[33m/\u001b[0m\u001b[33m250\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m115\u001b[0m\u001b[33m76\u001b[0m\u001b[33m',\u001b[0m\u001b[33m vector\u001b[0m\u001b[33m_db\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m='\u001b[0m\u001b[33myour\u001b[0m\u001b[33m_vector\u001b[0m\u001b[33m_db\u001b[0m\u001b[33m_id\u001b[0m\u001b[33m')]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:ingest_document_to_vectordb Args:{'source': 'https://arxiv.org/pdf/2503.11576', 'vector_db_id': 'your_vector_db_id'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:ingest_document_to_vectordb Response:{\"type\":\"text\",\"text\":\"2503.11576v1.pdf\",\"annotations\":null}\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mWhat\u001b[0m\u001b[33m is\u001b[0m\u001b[33m the\u001b[0m\u001b[33m metadata\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m ing\u001b[0m\u001b[33mested\u001b[0m\u001b[33m document\u001b[0m\u001b[33m?\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">User&gt; Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[36mUser> Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m[k\u001b[0m\u001b[33mnowledge\u001b[0m\u001b[33m_search\u001b[0m\u001b[33m(query\u001b[0m\u001b[33m=\"\u001b[0m\u001b[33msystem\u001b[0m\u001b[33m vs\u001b[0m\u001b[33m human\u001b[0m\u001b[33m analysis\u001b[0m\u001b[33m of\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m\")]\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'system vs human analysis of layout'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1:\\nDocument_id:17402\\nContent: C.2 Layout analysis samples\\nTo illustrate and compare the visual grounding capabilities of SmolDocling, a set of sample predictions with SmolDocling and Qwen2.5-VL from DocLayNet [70] is shown in Table 7. Note that the element location results are independent from the correct reproduction of document content and structure.\\nTable 7: Visualizations of layout output from SmolDocling and QwenVL2.5 compared to the DocLayNet ground truth. Examples are chosen to be representative of different layout styles and features. The prediction results however do not represent a generalizable measure of the model's performance on inputs with similar features. (1) Multi-column pages are handled by SmolDocling and Qwen2.5-VL, with some recall errors in the latter. (2) A manual page with terminal output shows poor bounding box recall on SmolDocling and label confusion in Qwen2.5-VL. (3) Lists with nesting are handled well in SmolDocling but confuse Qwen2.5-VL. (4) Both SmolDocling and Qwen2.5-VL reconstruct equations well, however with different annotation conventions on including or excluding the equation index. (5) On a portrait page with colorful elements and gradient background, SmolDocling creates less accurate bounding-boxes and Qwen2.5-VL suffers low recall. (6) On a report page with tables and diagrams, SmolDocling output exhibits some repetition loop, fabricating non-existent text cells (bottom left), while Qwen2.5-VL is confused between tables and pictures.\\n\", type='text'), TextContentItem(text=\"Result 2:\\nDocument_id:17402\\nContent: C.2 Layout analysis samples\\nTable 7: Visualizations of layout output from SmolDocling and QwenVL2.5 compared to the DocLayNet ground truth. Examples are chosen to be representative of different layout styles and features. The prediction results however do not represent a generalizable measure of the model's performance on inputs with similar features. (1) Multi-column pages are handled by SmolDocling and Qwen2.5-VL, with some recall errors in the latter. (2) A manual page with terminal output shows poor bounding box recall on SmolDocling and label confusion in Qwen2.5-VL. (3) Lists with nesting are handled well in SmolDocling but confuse Qwen2.5-VL. (4) Both SmolDocling and Qwen2.5-VL reconstruct equations well, however with different annotation conventions on including or excluding the equation index. (5) On a portrait page with colorful elements and gradient background, SmolDocling creates less accurate bounding-boxes and Qwen2.5-VL suffers low recall. (6) On a report page with tables and diagrams, SmolDocling output exhibits some repetition loop, fabricating non-existent text cells (bottom left), while Qwen2.5-VL is confused between tables and pictures.\\n\", type='text'), TextContentItem(text=\"Result 3:\\nDocument_id:17402\\nContent: C.2 Layout analysis samples\\nTable 7: Visualizations of layout output from SmolDocling and QwenVL2.5 compared to the DocLayNet ground truth. Examples are chosen to be representative of different layout styles and features. The prediction results however do not represent a generalizable measure of the model's performance on inputs with similar features. (1) Multi-column pages are handled by SmolDocling and Qwen2.5-VL, with some recall errors in the latter. (2) A manual page with terminal output shows poor bounding box recall on SmolDocling and label confusion in Qwen2.5-VL. (3) Lists with nesting are handled well in SmolDocling but confuse Qwen2.5-VL. (4) Both SmolDocling and Qwen2.5-VL reconstruct equations well, however with different annotation conventions on including or excluding the equation index. (5) On a portrait page with colorful elements and gradient background, SmolDocling creates less accurate bounding-boxes and Qwen2.5-VL suffers low recall. (6) On a report page with tables and diagrams, SmolDocling output exhibits some repetition loop, fabricating non-existent text cells (bottom left), while Qwen2.5-VL is confused between tables and pictures.\\n\", type='text'), TextContentItem(text='Result 4:\\nDocument_id:17402\\nContent: 5.2 Quantitative Results\\nTo enable valid cross-model comparisons with SmolDocling, we addressed multiple dataset and harmonization challenges:\\n· Public evaluation datasets lack comprehensive multi-task annotations with diverse layouts. We augmented DocLayNet [70] with text content and reading-order to enable evaluation of both layout analysis and text recognition. Table structure, code listing, formula and chart recognition are evaluated on specific datasets.\\n· Different models each follow their own conventions for output markup (e.g., Markdown, HTML, DocTags) and produce annotations which do not fully align in semantics. For example, SmolDocling is trained to produce 14 distinct layout-class tags via DocTags , while Qwen2.5-VL combines standard HTML tags and a few explicit class attributes with only partially compatible definitions. GOT and Nougat output Markdown, which can express paragraphs, lists and tables without location annotation.\\n- · Input resolution of page images can affect model performance, but is rarely specified in literature. We standardized at 144 DPI (dots per inch), which reproduces enough detail to be perfectly legible by humans while keeping resource demand for inference reasonable.\\nText Recognition (OCR). We first evaluate text recognition accuracy of SmolDocling, compared to Nougat, GOT and Qwen2.5-VL (Table 2), to verify accurate character transcription and reading order. The text-similarity metrics, adopted from [12] require plain formatting without non-textual elements. For different tasks, we conduct evaluations on diverse content types: full-page documents (from DocLayNet test set) in Markdown format, code snippets (from our SynthCodeNet dataset) in plaintext, and equations (from Im2Latex-230k [21]) in LaTeX format. For Qwen2.5-VL and SmolDocling, we convert HTML or DocTags output to Markdown while preserving formatting where possible.\\n', type='text'), TextContentItem(text='Result 5:\\nDocument_id:17402\\nContent: 1 Introduction\\nFor decades, converting complex digital documents into a structured, machine-processable format has been a significant technical challenge. This challenge primarily stems from the substantial variability in document layouts and styles, as well as the inherently opaque nature of the widely used PDF format, which is optimized for printing rather than semantic parsing. Intricate layout styles and visually challenging elements such as forms, tables, and complex charts can significantly impact the reading order and general understanding of documents. These problems have driven extensive research and development across multiple domains of computer science. On one hand, sophisticated ensemble systems emerged, which decompose the conversion problem into several sub-tasks (e.g. OCR, layout analysis, table structure recognition, classification) and tackle each sub-task independently. Although such systems can achieve high-quality results for many document types while maintaining relatively low computational demands, they are often difficult to tune and generalize.\\nOn the other hand, in the recent past, great interest has developed around large foundational multimodal models that can solve the whole conversion task in one shot, while simultaneously offering flexible querying and parametrization through prompts. This approach has been made possible by the advent of multimodal pre-training of large visionlanguage models (LVLMs), which opened up a vast array of opportunities for leveraging diverse data sources, including PDF documents. However, the literature on this topic highlights a significant gap in the availability of high-quality and open-access datasets suitable for training robust multi-modal models for the task of document understanding. Furthermore, relying on LVLMs may introduce common issues associated with such models, including hallucinations and the use of significant computational resources, making them impractical from both a quality and cost perspective.\\nFigure 1: SmolDocling/SmolVLM architecture. SmolDocling converts images of document pages to DocTags sequences. First, input images are encoded using a vision encoder and reshaped via projection and pooling. Then, the projected embeddings are concatenated with the text embeddings of the user prompt, possibly with interleaving. Finally, the sequence is used by an LLM to autoregressively predict the DocTags sequence.\\n', type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"system vs human analysis of layout\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m appears\u001b[0m\u001b[33m that\u001b[0m\u001b[33m both\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m have\u001b[0m\u001b[33m their\u001b[0m\u001b[33m strengths\u001b[0m\u001b[33m and\u001b[0m\u001b[33m weaknesses\u001b[0m\u001b[33m when\u001b[0m\u001b[33m analyzing\u001b[0m\u001b[33m layouts\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Sm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m and\u001b[0m\u001b[33m Q\u001b[0m\u001b[33mwen\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33m-V\u001b[0m\u001b[33mL\u001b[0m\u001b[33m are\u001b[0m\u001b[33m compared\u001b[0m\u001b[33m to\u001b[0m\u001b[33m Doc\u001b[0m\u001b[33mL\u001b[0m\u001b[33may\u001b[0m\u001b[33mNet\u001b[0m\u001b[33m in\u001b[0m\u001b[33m terms\u001b[0m\u001b[33m of\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m analysis\u001b[0m\u001b[33m performance\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mSm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m is\u001b[0m\u001b[33m able\u001b[0m\u001b[33m to\u001b[0m\u001b[33m handle\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-column\u001b[0m\u001b[33m pages\u001b[0m\u001b[33m well\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m has\u001b[0m\u001b[33m some\u001b[0m\u001b[33m recall\u001b[0m\u001b[33m errors\u001b[0m\u001b[33m in\u001b[0m\u001b[33m handling\u001b[0m\u001b[33m lists\u001b[0m\u001b[33m with\u001b[0m\u001b[33m nesting\u001b[0m\u001b[33m.\u001b[0m\u001b[33m It\u001b[0m\u001b[33m also\u001b[0m\u001b[33m struggles\u001b[0m\u001b[33m with\u001b[0m\u001b[33m portrait\u001b[0m\u001b[33m pages\u001b[0m\u001b[33m that\u001b[0m\u001b[33m have\u001b[0m\u001b[33m colorful\u001b[0m\u001b[33m elements\u001b[0m\u001b[33m and\u001b[0m\u001b[33m gradient\u001b[0m\u001b[33m backgrounds\u001b[0m\u001b[33m.\u001b[0m\u001b[33m On\u001b[0m\u001b[33m the\u001b[0m\u001b[33m other\u001b[0m\u001b[33m hand\u001b[0m\u001b[33m,\u001b[0m\u001b[33m Sm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m handles\u001b[0m\u001b[33m lists\u001b[0m\u001b[33m with\u001b[0m\u001b[33m nesting\u001b[0m\u001b[33m well\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m conf\u001b[0m\u001b[33muses\u001b[0m\u001b[33m tables\u001b[0m\u001b[33m and\u001b[0m\u001b[33m pictures\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mQ\u001b[0m\u001b[33mwen\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33m-V\u001b[0m\u001b[33mL\u001b[0m\u001b[33m has\u001b[0m\u001b[33m poor\u001b[0m\u001b[33m bounding\u001b[0m\u001b[33m box\u001b[0m\u001b[33m recall\u001b[0m\u001b[33m on\u001b[0m\u001b[33m manual\u001b[0m\u001b[33m pages\u001b[0m\u001b[33m with\u001b[0m\u001b[33m terminal\u001b[0m\u001b[33m output\u001b[0m\u001b[33m and\u001b[0m\u001b[33m is\u001b[0m\u001b[33m confused\u001b[0m\u001b[33m between\u001b[0m\u001b[33m tables\u001b[0m\u001b[33m and\u001b[0m\u001b[33m pictures\u001b[0m\u001b[33m.\u001b[0m\u001b[33m However\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m reconstruct\u001b[0m\u001b[33ms\u001b[0m\u001b[33m equations\u001b[0m\u001b[33m well\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m with\u001b[0m\u001b[33m different\u001b[0m\u001b[33m annotation\u001b[0m\u001b[33m conventions\u001b[0m\u001b[33m compared\u001b[0m\u001b[33m to\u001b[0m\u001b[33m Doc\u001b[0m\u001b[33mL\u001b[0m\u001b[33may\u001b[0m\u001b[33mNet\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m results\u001b[0m\u001b[33m suggest\u001b[0m\u001b[33m that\u001b[0m\u001b[33m while\u001b[0m\u001b[33m both\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m have\u001b[0m\u001b[33m their\u001b[0m\u001b[33m strengths\u001b[0m\u001b[33m and\u001b[0m\u001b[33m weaknesses\u001b[0m\u001b[33m,\u001b[0m\u001b[33m Sm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m seems\u001b[0m\u001b[33m to\u001b[0m\u001b[33m perform\u001b[0m\u001b[33m better\u001b[0m\u001b[33m in\u001b[0m\u001b[33m handling\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-column\u001b[0m\u001b[33m pages\u001b[0m\u001b[33m and\u001b[0m\u001b[33m lists\u001b[0m\u001b[33m with\u001b[0m\u001b[33m nesting\u001b[0m\u001b[33m,\u001b[0m\u001b[33m while\u001b[0m\u001b[33m Q\u001b[0m\u001b[33mwen\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m5\u001b[0m\u001b[33m-V\u001b[0m\u001b[33mL\u001b[0m\u001b[33m performs\u001b[0m\u001b[33m better\u001b[0m\u001b[33m in\u001b[0m\u001b[33m reconstruct\u001b[0m\u001b[33ming\u001b[0m\u001b[33m equations\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mIt\u001b[0m\u001b[33m's\u001b[0m\u001b[33m also\u001b[0m\u001b[33m worth\u001b[0m\u001b[33m noting\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m results\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m challenges\u001b[0m\u001b[33m of\u001b[0m\u001b[33m comparing\u001b[0m\u001b[33m layout\u001b[0m\u001b[33m analysis\u001b[0m\u001b[33m models\u001b[0m\u001b[33m,\u001b[0m\u001b[33m as\u001b[0m\u001b[33m they\u001b[0m\u001b[33m use\u001b[0m\u001b[33m different\u001b[0m\u001b[33m conventions\u001b[0m\u001b[33m for\u001b[0m\u001b[33m output\u001b[0m\u001b[33m markup\u001b[0m\u001b[33m and\u001b[0m\u001b[33m may\u001b[0m\u001b[33m have\u001b[0m\u001b[33m varying\u001b[0m\u001b[33m levels\u001b[0m\u001b[33m of\u001b[0m\u001b[33m detail\u001b[0m\u001b[33m in\u001b[0m\u001b[33m their\u001b[0m\u001b[33m annotations\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mOverall\u001b[0m\u001b[33m,\u001b[0m\u001b[33m it\u001b[0m\u001b[33m appears\u001b[0m\u001b[33m that\u001b[0m\u001b[33m both\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m have\u001b[0m\u001b[33m room\u001b[0m\u001b[33m for\u001b[0m\u001b[33m improvement\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m Sm\u001b[0m\u001b[33mol\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m seems\u001b[0m\u001b[33m to\u001b[0m\u001b[33m be\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m robust\u001b[0m\u001b[33m performer\u001b[0m\u001b[33m in\u001b[0m\u001b[33m certain\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Ingest the document https://arxiv.org/pdf/2503.11576\",\n",
    "    \"Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    console.print(f\"\\n[cyan]User> {prompt}[/cyan]\")\n",
    "    \n",
    "    # create a new turn with a new session ID for each prompt\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
    "        stream=settings.stream,\n",
    "    )\n",
    "    \n",
    "    # print the response, including tool calls output\n",
    "    if settings.stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        pprint(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b00480",
   "metadata": {},
   "source": [
    "## What happened?\n",
    "\n",
    "The code above executed a chat interaction with an agent.\n",
    "\n",
    "With the first message, we instruct the agent to ingest the document. The model, performing its reasoning, plans decides to call the Docling tool for converting the document.\n",
    "\n",
    "With the second message, we ask the agent to search the ingested content.\n",
    "Note how the model decides on its own which one is a good query for search the relevant chunks in the vector database. Compared to the previous labs, the retrieval is not done with the exact use query. This is interpreted and tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa41de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4c772",
   "metadata": {},
   "source": [
    "# ReAct Agent\n",
    "\n",
    "In the following section we use the reasoning agent `ReActAgent`. In this scenario, the model orchestrator the tools execution is reasoning on the sequence of tools to be executed in order to perform the task.\n",
    "\n",
    "This allows to have a single user query which triggers multiple independent steps, e.g.\n",
    "\n",
    "1. Ingest the documents\n",
    "2. Run a search query on the documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=settings.vdb_embedding,\n",
    "    embedding_dimension=settings.vdb_embedding_dimension,\n",
    "    provider_id=settings.vdb_provider,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = ReActAgent(\n",
    "            client=client,\n",
    "            model=settings.inference_model_id,\n",
    "            tools=[\n",
    "                dict(\n",
    "                    name=\"mcp::docling-llamastack\",\n",
    "                    args={\n",
    "                        \"vector_db_id\": vector_db_id,\n",
    "                    },\n",
    "                ),\n",
    "                dict(\n",
    "                    name=\"builtin::rag/knowledge_search\",\n",
    "                    args={\n",
    "                        \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
    "                    },\n",
    "                )\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": ReActOutput.model_json_schema(),\n",
    "            },\n",
    "            sampling_params=sampling_params,\n",
    "        )\n",
    "user_prompts = [\n",
    "    \"I would like to summarize the statements of the authors of https://arxiv.org/pdf/2503.11576 on how does SmolDocling compare to humans when analyzing the layout.\"\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    console.print(f\"[cyan]Processing user query: {prompt}[/cyan]\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
    "        stream=settings.stream\n",
    "    )\n",
    "    if settings.stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        pprint(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e85043",
   "metadata": {},
   "source": [
    "## What happened?\n",
    "\n",
    "Compared to the previous agent, here the model used advanced reasining for creating a plan of actions needed to perform the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f52cf8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e863a",
   "metadata": {},
   "source": [
    "# Summary and Next Steps\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "Congratulations! You've successfully used Docling in an agentic framework. Here's what you've learned:\n",
    "\n",
    "- **Lab 1**: Document structure preservation enables everything else\n",
    "- **Lab 2**: Intelligent chunking optimizes retrieval quality\n",
    "- **Lab 3**: Visual grounding transforms RAG into transparent AI\n",
    "- **Lab 4**: Run Docling as MCP tool with Llama Stack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b707b4",
   "metadata": {},
   "source": [
    "## Next Steps: Where to Go from Here\n",
    "\n",
    "### Immediate actions\n",
    "\n",
    "1. **Experiment with your documents**\n",
    "   - Try documents with complex layouts\n",
    "   - Test with technical diagrams and charts\n",
    "   - Process multi-page reports with mixed content\n",
    "\n",
    "2. **Connect more agents**\n",
    "   - Try connecting more tools\n",
    "   - Search the documents to ingest via metadata\n",
    "   - Search the web for relevant documents\n",
    "   - Extract information from the documents\n",
    "\n",
    "3. **More ways to interact with tools**\n",
    "   - Use the Llama Stack playground UI for chatting with the agents\n",
    "   - Use other frameworks and ecosystems like Claude Desktop, BeeAI, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62057f76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec8b06",
   "metadata": {},
   "source": [
    "## Resources for Continued Learning\n",
    "\n",
    "### Official Documentation\n",
    "- **[Docling Documentation](https://github.com/docling-project/docling)**: Latest features and updates\n",
    "\n",
    "### Community Resources\n",
    "- Join the Docling community on GitHub\n",
    "- Share your implementations\n",
    "- Contribute improvements back to the project\n",
    "\n",
    "### Related Topics to Explore\n",
    "- Document Layout Analysis\n",
    "- Multimodal Embeddings\n",
    "- Visual Question Answering\n",
    "- Explainable AI Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c2c77",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b456ab9",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "You've completed an incredible journey from basic document conversion to building a sophisticated, transparent AI system. The combination of Docling's document understanding with AI frameworks like Langchain, Llama Stack and MCP allows to build powerful applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8a3b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
