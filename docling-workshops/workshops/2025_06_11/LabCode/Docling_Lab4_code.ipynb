{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccb6e4d",
   "metadata": {},
   "source": [
    "# Docling as MCP tool with Llama Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c4bb8",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd21e20",
   "metadata": {},
   "source": [
    "### Technologies We'll Use\n",
    "\n",
    "Building on our previous labs, we'll add:\n",
    "\n",
    "1. **[Docling](https://docling-project.github.io/docling/):** An open-source toolkit used to parse and convert documents.\n",
    "2. **[MCP](https://modelcontextprotocol.io)**: The model context protocol for creating a tool.\n",
    "3. **[Llama Stack](https://llama-stack.readthedocs.io/)**: Framework for building generative AI applications.\n",
    "4. Agentic RAG: Use reasing and tools for an enhanced RAG flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12622cca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ade1e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we begin, ensure you have:\n",
    "- Completed Labs 1 (or equivalent Docling knowledge)\n",
    "- Python >=3.10 installed\n",
    "- Ollama installed\n",
    "- Podman installed (or Docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcf6a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12afefd",
   "metadata": {},
   "source": [
    "## Installation and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \\\n",
    "    llama-stack-client==0.2.0 \\\n",
    "    pydantic \\\n",
    "    pydantic_settings \\\n",
    "    rich\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0447576",
   "metadata": {},
   "source": [
    "Now let's import the essential modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from pydantic import NonNegativeFloat\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "from rich.console import Console\n",
    "from rich.pretty import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb231c7",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf694d4",
   "metadata": {},
   "source": [
    "To see detailed information about the document processing and chunking operations, we'll configure INFO log level.\n",
    "\n",
    "NOTE: It is okay to skip running this cell if you prefer less verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb01a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.hasHandlers():  \n",
    "    logger.setLevel(logging.INFO)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d862854",
   "metadata": {},
   "source": [
    "### Setting up\n",
    "\n",
    "In the following blocks we setup the environment needed for connecting to llama stack and use it as agentic framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings(BaseSettings):\n",
    "    base_url: str\n",
    "\n",
    "    vdb_provider: str\n",
    "    vdb_embedding: str\n",
    "    vdb_embedding_dimension: int\n",
    "    vdb_embedding_window: int\n",
    "\n",
    "    inference_model_id: str\n",
    "    max_tokens: int\n",
    "    temperature: NonNegativeFloat\n",
    "    top_p: float\n",
    "    stream: bool\n",
    "\n",
    "    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(\n",
    "    base_url=\"http://localhost:8321\",\n",
    "    inference_model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    "    top_p=0.95,\n",
    "    stream=True,\n",
    "    vdb_provider=\"faiss\",\n",
    "    vdb_embedding=\"all-MiniLM-L6-v2\",\n",
    "    vdb_embedding_dimension=384,\n",
    "    vdb_embedding_window=256,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if settings.temperature > 0.0:\n",
    "    strategy = {\n",
    "        \"type\": \"top_p\",\n",
    "        \"temperature\": settings.temperature,\n",
    "        \"top_p\": settings.top_p,\n",
    "    }\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": settings.max_tokens,\n",
    "}\n",
    "\n",
    "print(sampling_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c87c25",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f1548",
   "metadata": {},
   "source": [
    "## Launch Llama Stack\n",
    "\n",
    "Within this lab we will interact with a Llama Stack backend. We have chosen the Ollama distribution which allows to easily get started on a local environment.\n",
    "\n",
    "### Fetch the models\n",
    "\n",
    "In a terminal window use the following command for fetching the models required for running.\n",
    "\n",
    "```bash\n",
    "export INFERENCE_MODEL=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# ollama names this model differently, and we must use the ollama name when loading the model\n",
    "export OLLAMA_INFERENCE_MODEL=\"llama3.2:3b-instruct-fp16\"\n",
    "ollama run $OLLAMA_INFERENCE_MODEL --keepalive 60m\n",
    "  ```\n",
    "\n",
    "\n",
    "### Start the Llama Stack container\n",
    "\n",
    "In a new terminal window use the following command to run the Llama Stack server.\n",
    "\n",
    "```bash\n",
    "# make a working directory which will be used by the container\n",
    "mkdir -p ~/.llama\n",
    "\n",
    "# launch llama stack\n",
    "export LLAMA_STACK_PORT=8321\n",
    "podman run \\\n",
    "  -it \\\n",
    "  --pull always \\\n",
    "  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \\\n",
    "  -v ~/.llama:/root/.llama \\\n",
    "  llamastack/distribution-ollama \\\n",
    "  --port $LLAMA_STACK_PORT \\\n",
    "  --env INFERENCE_MODEL=$INFERENCE_MODEL \\\n",
    "  --env OLLAMA_URL=http://host.containers.internal:11434\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03360750",
   "metadata": {},
   "source": [
    "Next we can use the `LlamaStackClient` within this notebook validate the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d42db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LlamaStackClient(base_url=settings.base_url)\n",
    "print(f\"Connected to Llama Stack server @ {client.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dcba84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d54948",
   "metadata": {},
   "source": [
    "## Launch the MCP tool\n",
    "\n",
    "MCP allows to connect custom tools (like Docling) within an agentic framework. In this lab we will use an MCP tool which allows to\n",
    "1. Convert documents using Docling\n",
    "2. Ingest them into a Llama Stack vector DB instance.\n",
    "\n",
    "_You can inspect how a tool is created by looking at the file [Docling_Lab4_tool.py](./Docling_Lab4_tool.py)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14983d2a",
   "metadata": {},
   "source": [
    "We already packaged the tool into a working container image which is ready for you to try out.\n",
    "\n",
    "**Launch the Docling Llama Stack MCP tool** by running the following command in a new terminal window.\n",
    "\n",
    "```bash\n",
    "podman run \\\n",
    "  -it \\\n",
    "  --pull always \\\n",
    "  -p 8000:8000 \\\n",
    "  quay.io/docling-project/lab-demo-docling-llamstack-mcp \\\n",
    "  --env DOCLING_MCP_LLAMA_STACK_URL=http://host.containers.internal:8321\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c3b7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26d641",
   "metadata": {},
   "source": [
    "### Validate tools available in our llama-stack instance\n",
    "\n",
    "When an instance of llama-stack is redeployed your tools need to re-registered. Also if a tool is already registered with a llama-stack instance, if you try to register one with the same `toolgroup_id`, llama-stack will throw you an error.\n",
    "\n",
    "For this reason it is recommended to include some code to validate your tools and toolgroups. This is where the `mcp_url` comes into play. The following code will check that the `mcp::docling-llamastack` tool is registered, or it will be registered directly from the mcp url.\n",
    "\n",
    "If you are running the MCP server from source, the default value for this is: `http://localhost:8000/sse`.\n",
    "\n",
    "If you are running the MCP server from a container, the default value for this is: `http://host.containers.internal:8000/sse`.\n",
    "\n",
    "Make sure to pass the corresponding MCP URL for the server you are trying to register/validate tools for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd3787",
   "metadata": {},
   "outputs": [],
   "source": [
    "docling_mcp_url = \"http://host.containers.internal:8000/sse\"\n",
    "\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "\n",
    "if \"mcp::docling-llamastack\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::docling-llamastack\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":docling_mcp_url},\n",
    "    )\n",
    "\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "logger.info(f\"Your Llama Stack server is already registered with the following tool groups @ {set(registered_toolgroups)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f79fb",
   "metadata": {},
   "source": [
    "## Ingest + RAG-aware agent\n",
    "\n",
    "- Initialize the collection in the vectordb\n",
    "- Initialize the agent the required tools:\n",
    "    - Docling Ingest will be responsible to take care of instructions like \"Ingest the document https://arxiv.org/pdf/2503.11576\".\n",
    "    - RAG/Knowledge search will respond to user queries by running RAG on the documents ingested in the vectordb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fe883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the vectordb collection to use\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=settings.vdb_embedding,\n",
    "    embedding_dimension=settings.vdb_embedding_dimension,\n",
    "    provider_id=settings.vdb_provider,\n",
    ")\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=settings.inference_model_id,\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    sampling_params=sampling_params,\n",
    "    tools=[\n",
    "        dict(\n",
    "            name=\"mcp::docling-llamastack\",\n",
    "            args={\n",
    "                \"vector_db_id\": vector_db_id,\n",
    "            },\n",
    "        ),\n",
    "        dict(\n",
    "            name=\"builtin::rag/knowledge_search\",\n",
    "            args={\n",
    "                \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "v=client.vector_dbs.retrieve(vector_db_id)\n",
    "client.vector_io.query(vector_db_id=vector_db_id, query=\"docling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c653cc3",
   "metadata": {},
   "source": [
    "## Executing ingest and RAG queries\n",
    "\n",
    "- For each prompt, initialize a new agent session, execute a turn during which a retrieval call may be requested, and output the reply received from the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Ingest the document https://arxiv.org/pdf/2503.11576\",\n",
    "    \"Lookup the documents to answer the question: How does the system compare to humans when analyzing the layout?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    console.print(f\"\\n[cyan]User> {prompt}[/cyan]\")\n",
    "    \n",
    "    # create a new turn with a new session ID for each prompt\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
    "        stream=settings.stream,\n",
    "    )\n",
    "    \n",
    "    # print the response, including tool calls output\n",
    "    if settings.stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        pprint(response.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b00480",
   "metadata": {},
   "source": [
    "## What happened?\n",
    "\n",
    "The code above executed a chat interaction with an agent.\n",
    "\n",
    "With the first message, we instruct the agent to ingest the document. The model, performing its reasoning, plans decides to call the Docling tool for converting the document.\n",
    "\n",
    "With the second message, we ask the agent to search the ingested content.\n",
    "Note how the model decides on its own which one is a good query for search the relevant chunks in the vector database. Compared to the previous labs, the retrieval is not done with the exact use query. This is interpreted and tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa41de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4c772",
   "metadata": {},
   "source": [
    "# ReAct Agent\n",
    "\n",
    "In the following section we use the reasoning agent `ReActAgent`. In this scenario, the model orchestrator the tools execution is reasoning on the sequence of tools to be executed in order to perform the task.\n",
    "\n",
    "This allows to have a single user query which triggers multiple independent steps, e.g.\n",
    "\n",
    "1. Ingest the documents\n",
    "2. Run a search query on the documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "\n",
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=settings.vdb_embedding,\n",
    "    embedding_dimension=settings.vdb_embedding_dimension,\n",
    "    provider_id=settings.vdb_provider,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = ReActAgent(\n",
    "            client=client,\n",
    "            model=settings.inference_model_id,\n",
    "            tools=[\n",
    "                dict(\n",
    "                    name=\"mcp::docling-llamastack\",\n",
    "                    args={\n",
    "                        \"vector_db_id\": vector_db_id,\n",
    "                    },\n",
    "                ),\n",
    "                dict(\n",
    "                    name=\"builtin::rag/knowledge_search\",\n",
    "                    args={\n",
    "                        \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
    "                    },\n",
    "                )\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": ReActOutput.model_json_schema(),\n",
    "            },\n",
    "            sampling_params=sampling_params,\n",
    "        )\n",
    "user_prompts = [\n",
    "    \"I would like to summarize the statements of the authors of https://arxiv.org/pdf/2503.11576 on how does SmolDocling compare to humans when analyzing the layout.\"\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    console.print(f\"[cyan]Processing user query: {prompt}[/cyan]\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=agent.create_session(f\"rag-session_{uuid.uuid4()}\"),\n",
    "        stream=settings.stream\n",
    "    )\n",
    "    if settings.stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        pprint(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e85043",
   "metadata": {},
   "source": [
    "## What happened?\n",
    "\n",
    "Compared to the previous agent, here the model used advanced reasining for creating a plan of actions needed to perform the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f52cf8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e863a",
   "metadata": {},
   "source": [
    "# Summary and Next Steps\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "Congratulations! You've successfully used Docling in an agentic framework. Here's what you've learned:\n",
    "\n",
    "- **Lab 1**: Document structure preservation enables everything else\n",
    "- **Lab 2**: Intelligent chunking optimizes retrieval quality\n",
    "- **Lab 3**: Visual grounding transforms RAG into transparent AI\n",
    "- **Lab 4**: Run Docling as MCP tool with Llama Stack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b707b4",
   "metadata": {},
   "source": [
    "## Next Steps: Where to Go from Here\n",
    "\n",
    "### Immediate actions\n",
    "\n",
    "1. **Experiment with your documents**\n",
    "   - Try documents with complex layouts\n",
    "   - Test with technical diagrams and charts\n",
    "   - Process multi-page reports with mixed content\n",
    "\n",
    "2. **Connect more agents**\n",
    "   - Try connecting more tools\n",
    "   - Search the documents to ingest via metadata\n",
    "   - Search the web for relevant documents\n",
    "   - Extract information from the documents\n",
    "\n",
    "3. **More ways to interact with tools**\n",
    "   - Use the Llama Stack playground UI for chatting with the agents\n",
    "   - Use other frameworks and ecosystems like Claude Desktop, BeeAI, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62057f76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec8b06",
   "metadata": {},
   "source": [
    "## Resources for Continued Learning\n",
    "\n",
    "### Official Documentation\n",
    "- **[Docling Documentation](https://github.com/docling-project/docling)**: Latest features and updates\n",
    "\n",
    "### Community Resources\n",
    "- Join the Docling community on GitHub\n",
    "- Share your implementations\n",
    "- Contribute improvements back to the project\n",
    "\n",
    "### Related Topics to Explore\n",
    "- Document Layout Analysis\n",
    "- Multimodal Embeddings\n",
    "- Visual Question Answering\n",
    "- Explainable AI Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c2c77",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b456ab9",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "You've completed an incredible journey from basic document conversion to building a sophisticated, transparent AI system. The combination of Docling's document understanding with AI frameworks like Langchain, Llama Stack and MCP allows to build powerful applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8a3b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
